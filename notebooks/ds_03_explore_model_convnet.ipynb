{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdf0c256-3310-4718-880d-b7a3e22538fb",
   "metadata": {},
   "source": [
    "# Explore the EarlyConvNet model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c52de95-0516-45d7-9e5c-b3fa1a81a0e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-31 21:26:20.203161: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX512F, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from src.models import early_convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa70aad3-9241-4257-bc84-0020811c2e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"early_convnet\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " C1 (Conv2D)                 multiple                  888       \n",
      "                                                                 \n",
      " S2 (Subsampling)            multiple                  12        \n",
      "                                                                 \n",
      " C3 (Conv2D)                 multiple                  3472      \n",
      "                                                                 \n",
      " S4 (Subsampling)            multiple                  32        \n",
      "                                                                 \n",
      " C5 (Conv2D)                 multiple                  23080     \n",
      "                                                                 \n",
      " F6 (Dense)                  multiple                  287       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27771 (108.48 KB)\n",
      "Trainable params: 27771 (108.48 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac2d02d-34cd-4194-a82e-e1e3cd6df46c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e432247-4ce7-4761-9c0a-a0df605b53fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import src.data.datasets.deep_globe_2018\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11468dd-45af-4a8f-b066-5e79086c5cc0",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5015853-08a3-4e8b-8ee0-307097e66aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pipeline\n",
    "batch_size_images = 1\n",
    "batch_size_patches = 8\n",
    "img_size = 612\n",
    "patch_size = 64\n",
    "patch_size_annotation = 2\n",
    "patch_stride = 32\n",
    "\n",
    "## Training\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07693c9-cb1a-4cda-a2a2-2bb849754373",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a71f0b8c-cc25-4e7a-a19a-6d1fcc06771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(input_image):\n",
    "  input_image = tf.cast(input_image, tf.float32) / 255.0\n",
    "  return input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddb39cf8-1943-4ecb-9805-ca83bba81bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_index(image):\n",
    "    palette = [\n",
    "        [0, 255, 255],   # urban_land\n",
    "        [255, 255, 0],   # agriculture_land\n",
    "        [255, 0, 255],   # rangeland\n",
    "        [0, 255, 0],     # forest_land\n",
    "        [0, 0, 255],     # water\n",
    "        [255, 255, 255], # barren_land\n",
    "        [0, 0, 0]        # unknown\n",
    "    ]\n",
    "    \n",
    "    one_hot_map = []\n",
    "    for colour in palette:\n",
    "        class_map = tf.reduce_all(tf.equal(image, colour), axis=-1)\n",
    "        one_hot_map.append(class_map)\n",
    "    one_hot_map = tf.stack(one_hot_map, axis=-1)\n",
    "    one_hot_map = tf.cast(one_hot_map, tf.uint8)\n",
    "    indexed = tf.math.argmax(one_hot_map, axis=2)\n",
    "    indexed = tf.cast(indexed, dtype=tf.uint8)\n",
    "    indexed = tf.expand_dims(indexed, -1)\n",
    "\n",
    "    return indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "089a3e87-0c9d-4bce-84c6-1a590ed3b346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_patches_labels(datapoint, image_size, patch_size, patch_size_annotation, stride):\n",
    "    crop_fraction = patch_size_annotation / patch_size\n",
    "    \n",
    "    images = tf.image.resize(datapoint['image'], (image_size, image_size))\n",
    "    img_patches = tf.image.extract_patches(\n",
    "        images = images,\n",
    "        sizes = [1, patch_size, patch_size, 1],\n",
    "        strides = [1, stride, stride, 1],\n",
    "        rates = [1, 1, 1, 1],\n",
    "        padding = 'VALID'\n",
    "    )\n",
    "    img_patches_flat = tf.reshape(img_patches, shape=(-1, patch_size, patch_size, 3))\n",
    "\n",
    "    annotations = tf.map_fn(rgb_to_index, datapoint['segmentation_mask'])\n",
    "    annotations = tf.image.resize(annotations, (image_size, image_size), method='nearest')\n",
    "\n",
    "    ann_patches = tf.image.extract_patches(\n",
    "        images = annotations,\n",
    "        sizes = [1, patch_size, patch_size, 1],\n",
    "        strides = [1, stride, stride, 1],\n",
    "        rates = [1, 1, 1, 1],\n",
    "        padding = 'VALID'\n",
    "    )\n",
    "    ann_patches_flat = tf.reshape(ann_patches, shape=(-1, patch_size, patch_size, 1))\n",
    "    central_pixels = tf.image.central_crop(ann_patches_flat, crop_fraction)\n",
    "    dim = tf.reduce_prod(tf.shape(central_pixels)[1:])\n",
    "    central_pixels = tf.reshape(central_pixels, [-1, dim])\n",
    "    pixel_category_idx = tf.reduce_max(central_pixels, axis=1) # reduce_mode is probably preferred but I chose a simpler implementation\n",
    "\n",
    "    img_patches_flat = normalize(img_patches_flat)\n",
    "    pixel_category_one_hot = tf.one_hot(\n",
    "        pixel_category_idx,\n",
    "        depth = 7, # TODO: make depth configurable\n",
    "        on_value = 1,\n",
    "        off_value = -1\n",
    "    )\n",
    "\n",
    "    return img_patches_flat, pixel_category_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eedafc-7e04-4473-befa-099c24c63e11",
   "metadata": {},
   "source": [
    "## Optimizer, loss function, dataset and model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64db1600-8da7-41f1-96b0-ec53d115a1ec",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0dd10c20-20bd-4ce6-b7d6-05029047753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train, ds_valid, ds_test), ds_info = tfds.load(\n",
    "    name='deep_globe_2018',\n",
    "    download=False,\n",
    "    with_info=True,\n",
    "    split=['all_images[:7]', 'all_images[7:9]', 'all_images[9:10]']\n",
    ")\n",
    "train_batches = (\n",
    "    ds_train\n",
    "    .batch(batch_size_images)\n",
    "    .map(lambda x: load_patches_labels(x, img_size, patch_size, patch_size_annotation, patch_stride), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .unbatch() # Flatten the batches for training\n",
    "    .batch(batch_size_patches) # Rebatch patches as desired\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")\n",
    "# ds_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c599a68-032f-4d6c-b922-b7eb93852f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_batches = (\n",
    "#     ds_train\n",
    "#     .take(1)\n",
    "#     .batch(1)\n",
    "#     .map(lambda x: load_patches_labels(x, img_size, patch_size, patch_size_annotation, patch_stride), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "#     .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "# )\n",
    "# for i, m in train_batches.take(1):\n",
    "#     sample_images = i[1190:1210]\n",
    "#     sample_masks = m[1190:1210]\n",
    "#     print(i.shape, m.shape)\n",
    "#     # samples = list(zip(sample_images, sample_masks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2056e06e-23e5-4869-8355-69aa04e17fdf",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a387c081-533b-4e31-b4c7-878bbbc0d90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af38675-6e15-49af-99c6-c53eb2966862",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be50f53f-c5c9-43a3-a4a9-29734a88aac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acee141b-7c81-4472-b13c-911b94e21ac4",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b19c0e5-5da8-4476-868a-5d06d429b92a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = early_convnet.EarlyConvnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3121f9dc-fdac-4ecb-9673-9cc3bc0fe420",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build((None, patch_size,patch_size,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34e3afe-23aa-4e15-a529-175c2abe0fd8",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df60666-b205-42e2-8f13-48fd186080f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_patch_in_batch():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a05d84c-ec04-44fd-8919-67a6c896dc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "(8, 7, 7, 7)\n",
      "(8, 7)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__SquaredDifference_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [8,7,7,7] vs. [8,7] [Op:SquaredDifference] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(logits\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(cat_batch_train\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 11\u001b[0m     loss_value \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcat_batch_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss_value, model\u001b[38;5;241m.\u001b[39mtrainable_weights)\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(grads, model\u001b[38;5;241m.\u001b[39mtrainable_weights))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/src/losses.py:142\u001b[0m, in \u001b[0;36mLoss.__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall, tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mcontrol_status_ctx()\n\u001b[1;32m    140\u001b[0m     )\n\u001b[0;32m--> 142\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m in_mask \u001b[38;5;241m=\u001b[39m losses_utils\u001b[38;5;241m.\u001b[39mget_mask(y_pred)\n\u001b[1;32m    145\u001b[0m out_mask \u001b[38;5;241m=\u001b[39m losses_utils\u001b[38;5;241m.\u001b[39mget_mask(losses)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/src/losses.py:268\u001b[0m, in \u001b[0;36mLossFunctionWrapper.call\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m    261\u001b[0m     y_pred, y_true \u001b[38;5;241m=\u001b[39m losses_utils\u001b[38;5;241m.\u001b[39msqueeze_or_expand_dimensions(\n\u001b[1;32m    262\u001b[0m         y_pred, y_true\n\u001b[1;32m    263\u001b[0m     )\n\u001b[1;32m    265\u001b[0m ag_fn \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn, tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mcontrol_status_ctx()\n\u001b[1;32m    267\u001b[0m )\n\u001b[0;32m--> 268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mag_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/src/losses.py:1608\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m   1606\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(y_pred)\n\u001b[1;32m   1607\u001b[0m y_true \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(y_true, y_pred\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m-> 1608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mmean(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msquared_difference\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__SquaredDifference_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [8,7,7,7] vs. [8,7] [Op:SquaredDifference] name: "
     ]
    }
   ],
   "source": [
    "# Source: https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch (last accessed 31.12.2023)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    \n",
    "    for step, (img_batch_train, cat_batch_train) in enumerate(train_batches):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(img_batch_train, training=True)\n",
    "            print(logits.shape)\n",
    "            print(cat_batch_train.shape)\n",
    "            loss_value = loss_fn(cat_batch_train, logits)\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
