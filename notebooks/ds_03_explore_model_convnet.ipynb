{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdf0c256-3310-4718-880d-b7a3e22538fb",
   "metadata": {},
   "source": [
    "# Explore the EarlyConvNet model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c52de95-0516-45d7-9e5c-b3fa1a81a0e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-01 20:54:00.263133: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX512F, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from src.models import early_convnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac2d02d-34cd-4194-a82e-e1e3cd6df46c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e432247-4ce7-4761-9c0a-a0df605b53fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import src.data.datasets.deep_globe_2018\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11468dd-45af-4a8f-b066-5e79086c5cc0",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5015853-08a3-4e8b-8ee0-307097e66aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pipeline\n",
    "batch_size_images = 1\n",
    "batch_size_patches = 1\n",
    "img_size = 612\n",
    "patch_size = 80\n",
    "patch_size_annotation = 2\n",
    "patch_stride = 32\n",
    "\n",
    "## Training\n",
    "epochs = 3\n",
    "class_weights = {\n",
    "        0: 6.070,    # urban_land\n",
    "        1: 1.,       # agriculture_land\n",
    "        2: 5.559,    # rangeland\n",
    "        3: 4.128,    # forest_land\n",
    "        4: 15.176,   # water\n",
    "        5: 9.244,    # barren_land\n",
    "        6: 100.       # unknown - Note: not to scale with respect to the others but not that important for the overall classification\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07693c9-cb1a-4cda-a2a2-2bb849754373",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a71f0b8c-cc25-4e7a-a19a-6d1fcc06771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(input_image):\n",
    "  input_image = tf.cast(input_image, tf.float32) / 255.0\n",
    "  return input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddb39cf8-1943-4ecb-9805-ca83bba81bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_index(image):\n",
    "    palette = [\n",
    "        [0, 255, 255],   # urban_land\n",
    "        [255, 255, 0],   # agriculture_land\n",
    "        [255, 0, 255],   # rangeland\n",
    "        [0, 255, 0],     # forest_land\n",
    "        [0, 0, 255],     # water\n",
    "        [255, 255, 255], # barren_land\n",
    "        [0, 0, 0]        # unknown\n",
    "    ]\n",
    "    \n",
    "    one_hot_map = []\n",
    "    for colour in palette:\n",
    "        class_map = tf.reduce_all(tf.equal(image, colour), axis=-1)\n",
    "        one_hot_map.append(class_map)\n",
    "    one_hot_map = tf.stack(one_hot_map, axis=-1)\n",
    "    one_hot_map = tf.cast(one_hot_map, tf.uint8)\n",
    "    indexed = tf.math.argmax(one_hot_map, axis=2)\n",
    "    indexed = tf.cast(indexed, dtype=tf.uint8)\n",
    "    indexed = tf.expand_dims(indexed, -1)\n",
    "\n",
    "    return indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cc9cacd-fab0-4ee9-b65d-2bc9218b3457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mode(tensor):\n",
    "    y, idx, count = tf.unique_with_counts(tensor)\n",
    "    mode = y[tf.argmax(count)]\n",
    "    return mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "089a3e87-0c9d-4bce-84c6-1a590ed3b346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_patches_labels(datapoint, image_size, patch_size, patch_size_annotation, stride):\n",
    "    crop_fraction = patch_size_annotation / patch_size\n",
    "    \n",
    "    images = tf.image.resize(datapoint['image'], (image_size, image_size))\n",
    "    img_patches = tf.image.extract_patches(\n",
    "        images = images,\n",
    "        sizes = [1, patch_size, patch_size, 1],\n",
    "        strides = [1, stride, stride, 1],\n",
    "        rates = [1, 1, 1, 1],\n",
    "        padding = 'VALID'\n",
    "    )\n",
    "    img_patches_flat = tf.reshape(img_patches, shape=(-1, patch_size, patch_size, 3))\n",
    "\n",
    "    annotations = tf.map_fn(rgb_to_index, datapoint['segmentation_mask'])\n",
    "    annotations = tf.image.resize(annotations, (image_size, image_size), method='nearest')\n",
    "\n",
    "    ann_patches = tf.image.extract_patches(\n",
    "        images = annotations,\n",
    "        sizes = [1, patch_size, patch_size, 1],\n",
    "        strides = [1, stride, stride, 1],\n",
    "        rates = [1, 1, 1, 1],\n",
    "        padding = 'VALID'\n",
    "    )\n",
    "    ann_patches_flat = tf.reshape(ann_patches, shape=(-1, patch_size, patch_size, 1))\n",
    "    central_pixels = tf.image.central_crop(ann_patches_flat, crop_fraction)\n",
    "    dim = tf.reduce_prod(tf.shape(central_pixels)[1:])\n",
    "    central_pixels = tf.reshape(central_pixels, [-1, dim])\n",
    "\n",
    "    pixel_category_idx = tf.map_fn(reduce_mode, central_pixels)\n",
    "    # print(central_pixels)\n",
    "    # pixel_category_idx = tf.reduce_max(central_pixels, axis=1) # reduce_mode is probably preferred but I chose a simpler implementation\n",
    "\n",
    "    img_patches_flat = normalize(img_patches_flat)\n",
    "    pixel_category_one_hot = tf.one_hot(\n",
    "        pixel_category_idx,\n",
    "        depth = 7, # TODO: make depth configurable\n",
    "        on_value = 1,\n",
    "        off_value = -1\n",
    "    )\n",
    "\n",
    "    return img_patches_flat, pixel_category_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02daaf1c-c010-4fc5-a3f1-3bf059271748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(display_list):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "    print(len(display_list))\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eedafc-7e04-4473-befa-099c24c63e11",
   "metadata": {},
   "source": [
    "## Optimizer, loss function, dataset and model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64db1600-8da7-41f1-96b0-ec53d115a1ec",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dd10c20-20bd-4ce6-b7d6-05029047753c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-01 20:56:21.676489: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "(ds_train, ds_valid, ds_test), ds_info = tfds.load(\n",
    "    name='deep_globe_2018',\n",
    "    download=False,\n",
    "    with_info=True,\n",
    "    split=['all_images[:7]', 'all_images[7:9]', 'all_images[9:10]']\n",
    ")\n",
    "train_batches = (\n",
    "    ds_train\n",
    "    .batch(batch_size_images)\n",
    "    .map(lambda x: load_patches_labels(x, img_size, patch_size, patch_size_annotation, patch_stride), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .unbatch() # Flatten the batches for training\n",
    "    .batch(batch_size_patches) # Rebatch patches as desired\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")\n",
    "validation_batches = (\n",
    "    ds_valid\n",
    "    .batch(batch_size_images)\n",
    "    .map(lambda x: load_patches_labels(x, img_size, patch_size, patch_size_annotation, patch_stride), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .unbatch() # Flatten the batches for training\n",
    "    .batch(batch_size_patches) # Rebatch patches as desired\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")\n",
    "test_batches = (\n",
    "    ds_test\n",
    "    .batch(batch_size_images)\n",
    "    .map(lambda x: load_patches_labels(x, img_size, patch_size, patch_size_annotation, patch_stride), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ## unbatching not required for testing\n",
    "    # .unbatch() # Flatten the batches for training\n",
    "    # .batch(batch_size_patches) # Rebatch patches as desired\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")    \n",
    "# ds_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7957665b-7a5c-4f99-a496-066e6f724aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vals = np.fromiter(train_batches.map(lambda x, y: tf.argmax(y, axis=1)), float)\n",
    "\n",
    "plt.hist(vals)\n",
    "plt.xticks(range(10))\n",
    "plt.title('Label Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c68c660-e92e-435b-81b4-c7b8c5d560ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1., 2., 3., 4., 5.]), array([ 194, 1208,  206,  151,  121,  143]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(vals, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c599a68-032f-4d6c-b922-b7eb93852f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_batches = (\n",
    "#     ds_train\n",
    "#     .take(1)\n",
    "#     .batch(1)\n",
    "#     .map(lambda x: load_patches_labels(x, img_size, patch_size, patch_size_annotation, patch_stride), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "#     .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "# )\n",
    "# for i, m in train_batches.take(1):\n",
    "#     sample_images = i[1190:1210]\n",
    "#     sample_masks = m[1190:1210]\n",
    "#     print(i.shape, m.shape)\n",
    "#     # samples = list(zip(sample_images, sample_masks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2056e06e-23e5-4869-8355-69aa04e17fdf",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a387c081-533b-4e31-b4c7-878bbbc0d90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af38675-6e15-49af-99c6-c53eb2966862",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "be50f53f-c5c9-43a3-a4a9-29734a88aac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acee141b-7c81-4472-b13c-911b94e21ac4",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "df2aea0d-b768-4aa7-a158-4a9c41c16769",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = early_convnet.EarlyConvnet()\n",
    "model.build((None, patch_size, patch_size, 3))\n",
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss = loss_fn,\n",
    "    metrics = ['mse']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "db1bd610-0f65-412d-b996-74230db9a5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2023/2023 [==============================] - 34s 15ms/step - loss: 4.3885 - mse: 1.2522 - val_loss: 1.2646 - val_mse: 1.2646\n",
      "Epoch 2/3\n",
      "2023/2023 [==============================] - 32s 15ms/step - loss: 4.3564 - mse: 1.2475 - val_loss: 1.2593 - val_mse: 1.2593\n",
      "Epoch 3/3\n",
      "2023/2023 [==============================] - 33s 15ms/step - loss: 4.3467 - mse: 1.2418 - val_loss: 1.2501 - val_mse: 1.2501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7ff3b2781900>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_batches,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_batches,\n",
    "    class_weight=class_weights\n",
    "    #TODO: considering using class weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "30326003-c839-4c0a-97b8-eb32e0bceb08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 29ms/step\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input to reshape is a tensor with 34969 values, but the requested shape has 289 [Op:Reshape]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,m \u001b[38;5;129;01min\u001b[39;00m test_batches\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      2\u001b[0m     y \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(i)\n\u001b[0;32m----> 3\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m17\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m17\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     display(mask)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input to reshape is a tensor with 34969 values, but the requested shape has 289 [Op:Reshape]"
     ]
    }
   ],
   "source": [
    "for i,m in test_batches.take(1):\n",
    "    y = model.predict(i)\n",
    "    mask = tf.reshape(tf.argmax(input=y, axis=3), shape=(1, 17, 17, 1))\n",
    "    display(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b19c0e5-5da8-4476-868a-5d06d429b92a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 40, 40, 3)]       0         \n",
      "                                                                 \n",
      " C1 (Conv2D)                 (None, 34, 34, 6)         888       \n",
      "                                                                 \n",
      " S2 (Subsampling)            (None, 17, 17, 6)         12        \n",
      "                                                                 \n",
      " C3 (Conv2D)                 (None, 12, 12, 16)        3472      \n",
      "                                                                 \n",
      " S4 (Subsampling)            (None, 6, 6, 16)          32        \n",
      "                                                                 \n",
      " C5 (Conv2D)                 (None, 1, 1, 40)          23080     \n",
      "                                                                 \n",
      " F6 (Dense)                  (None, 1, 1, 7)           287       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27771 (108.48 KB)\n",
      "Trainable params: 27771 (108.48 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model.build((None, patch_size, patch_size, 3))\n",
    "# raw_input = (patch_size, patch_size, 3)\n",
    "# y = model(tf.ones(shape=(8,*raw_input))) \n",
    "model.build_graph(raw_input).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34e3afe-23aa-4e15-a529-175c2abe0fd8",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7a05d84c-ec04-44fd-8919-67a6c896dc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "(8, 1, 1, 7)\n",
      "(8, 7)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'The optimizer cannot recognize variable early_convnet_3/C1/kernel:0. This usually means you are trying to call the optimizer to update different parts of the model separately. Please call `optimizer.build(variables)` with the full list of trainable variables before the training loop or use legacy optimizer `tf.keras.optimizers.legacy.SGD.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m     loss_value \u001b[38;5;241m=\u001b[39m loss_fn(cat_batch_train, logits)\n\u001b[1;32m     12\u001b[0m grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss_value, model\u001b[38;5;241m.\u001b[39mtrainable_weights)\n\u001b[0;32m---> 13\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_weights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining loss (for one batch) at step \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;241m%\u001b[39m (step, \u001b[38;5;28mfloat\u001b[39m(loss_value))\n\u001b[1;32m     19\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:1230\u001b[0m, in \u001b[0;36mOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_gradients_aggregation \u001b[38;5;129;01mand\u001b[39;00m experimental_aggregate_gradients:\n\u001b[1;32m   1229\u001b[0m     grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate_gradients(grads_and_vars)\n\u001b[0;32m-> 1230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:652\u001b[0m, in \u001b[0;36m_BaseOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_weight_decay(trainable_variables)\n\u001b[1;32m    651\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(grads, trainable_variables))\n\u001b[0;32m--> 652\u001b[0m iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_apply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m trainable_variables:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:1260\u001b[0m, in \u001b[0;36mOptimizer._internal_apply_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mesh \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_with_dtensor:\n\u001b[1;32m   1257\u001b[0m     \u001b[38;5;66;03m# Skip any usage of strategy logic for DTensor\u001b[39;00m\n\u001b[1;32m   1258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_internal_apply_gradients(grads_and_vars)\n\u001b[0;32m-> 1260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_merge_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distributed_apply_gradients_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distribution_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/distribute/merge_call_interim.py:51\u001b[0m, in \u001b[0;36mmaybe_merge_call\u001b[0;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Maybe invoke `fn` via `merge_call` which may or may not be fulfilled.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03mThe caller of this utility function requests to invoke `fn` via `merge_call`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m  The return value of the `fn` call.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[0;32m---> 51\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m distribute_lib\u001b[38;5;241m.\u001b[39mget_replica_context()\u001b[38;5;241m.\u001b[39mmerge_call(\n\u001b[1;32m     54\u001b[0m       fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:1352\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn\u001b[0;34m(self, distribution, grads_and_vars, **kwargs)\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_step(grad, var)\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m grad, var \u001b[38;5;129;01min\u001b[39;00m grads_and_vars:\n\u001b[0;32m-> 1352\u001b[0m     \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_grad_to_update_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   1354\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ema:\n\u001b[1;32m   1357\u001b[0m     _, var_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgrads_and_vars)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:2992\u001b[0m, in \u001b[0;36mStrategyExtendedV2.update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   2989\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m   2990\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   2991\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m-> 2992\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2993\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2994\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replica_ctx_update(\n\u001b[1;32m   2995\u001b[0m       var, fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs, group\u001b[38;5;241m=\u001b[39mgroup)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:4062\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   4059\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, var, fn, args, kwargs, group):\n\u001b[1;32m   4060\u001b[0m   \u001b[38;5;66;03m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[1;32m   4061\u001b[0m   \u001b[38;5;66;03m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[0;32m-> 4062\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_non_slot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:4068\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update_non_slot\u001b[0;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[1;32m   4064\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_non_slot\u001b[39m(\u001b[38;5;28mself\u001b[39m, colocate_with, fn, args, kwargs, should_group):\n\u001b[1;32m   4065\u001b[0m   \u001b[38;5;66;03m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[1;32m   4066\u001b[0m   \u001b[38;5;66;03m# once that value is used for something.\u001b[39;00m\n\u001b[1;32m   4067\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[0;32m-> 4068\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_group:\n\u001b[1;32m   4070\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:596\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    595\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mUNSPECIFIED):\n\u001b[0;32m--> 596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:1349\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn.<locals>.apply_grad_to_update_var\u001b[0;34m(var, grad)\u001b[0m\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_step_xla(grad, var, \u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_key(var)))\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:233\u001b[0m, in \u001b[0;36m_BaseOptimizer._update_step\u001b[0;34m(self, gradient, variable)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_key(variable) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_dict:\n\u001b[0;32m--> 233\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m    234\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe optimizer cannot recognize variable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariable\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis usually means you are trying to call the optimizer to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate different parts of the model separately. Please call \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`optimizer.build(variables)` with the full list of trainable \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariables before the training loop or use legacy optimizer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.keras.optimizers.legacy.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m     )\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_step(gradient, variable)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'The optimizer cannot recognize variable early_convnet_3/C1/kernel:0. This usually means you are trying to call the optimizer to update different parts of the model separately. Please call `optimizer.build(variables)` with the full list of trainable variables before the training loop or use legacy optimizer `tf.keras.optimizers.legacy.SGD.'"
     ]
    }
   ],
   "source": [
    "# Source: https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch (last accessed 31.12.2023)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    \n",
    "    for step, (img_batch_train, cat_batch_train) in enumerate(train_batches):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(img_batch_train, training=True)\n",
    "            print(logits.shape)\n",
    "            print(cat_batch_train.shape)\n",
    "            loss_value = loss_fn(cat_batch_train, logits)\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
